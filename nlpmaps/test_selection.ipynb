{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_idf\n",
    "import ELMo\n",
    "import bag_of_words\n",
    "import BERT_updated\n",
    "import fasttext\n",
    "import wget\n",
    "import glove\n",
    "import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = pd.read_csv('/Users/andrewsimon/Desktop/Dow_dat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_embeddings(data, text, labels):\n",
    "    return tf_idf.generate_tfidf_embeddings(data, text, labels).drop(columns=labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BoW_embeddings(data, text, labels):\n",
    "    return bag_of_words.generate_bow_embeddings(data, text, labels).drop(columns=labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(data, text, labels):\n",
    "    return BERT_updated.generate_bert_embeddings(data, text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Word2Vec_embeddings(data, text):\n",
    "    return Word2Vec.get_embeddings(data, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings(data, text):\n",
    "    return ELMo.get_embeddings(data, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_embeddings(data, text, labels):\n",
    "    return fasttext.fasttext_embedding(data, text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embeddings(data, text, labels):\n",
    "    return glove.glove_embedding(data, text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_downstream(features, labels, test_size, random_state):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_model(X_train, X_test, y_train, y_test, n_estimators=1100, scoring_metric='accuracy'):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators).fit(X_train, y_train)\n",
    "\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    if scoring_metric == 'accuracy':\n",
    "        score = rf.score(X_test, y_test)\n",
    "        return score\n",
    "    elif scoring_metric == 'precision':\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        return precision\n",
    "    elif scoring_metric == 'recall':\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        return recall\n",
    "    elif scoring_metric == 'auc':\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        return auc_score\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_model(X_train, X_test, y_train, y_test, scoring_metric='accuracy'):\n",
    "    \n",
    "    clf_decision_tree = DecisionTreeClassifier()\n",
    "    clf_decision_tree.fit(X_train, y_train)\n",
    "\n",
    "    predictions = clf_decision_tree.predict(X_test)\n",
    "\n",
    "    if scoring_metric == 'accuracy':\n",
    "        score = accuracy_score(y_test, predictions)\n",
    "        return score\n",
    "    elif scoring_metric == 'precision':\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        return precision\n",
    "    elif scoring_metric == 'recall':\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        return recall\n",
    "    elif scoring_metric == 'auc':\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_model(X_train, X_test, y_train, y_test, scoring_metric='accuracy'):\n",
    "    \n",
    "    classifier = LogisticRegression( max_iter = 100000)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    predictions = classifier.predict(X_test)\n",
    "\n",
    "    if scoring_metric == 'accuracy':\n",
    "        score = accuracy_score(y_test, predictions)\n",
    "        return score\n",
    "    elif scoring_metric == 'precision':\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        return precision\n",
    "    elif scoring_metric == 'recall':\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        return recall\n",
    "    elif scoring_metric == 'auc':\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(X_train, X_test, y_train, y_test, scoring_metric='accuracy'):\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    if scoring_metric == 'accuracy':\n",
    "        score = accuracy_score(y_test, predictions)\n",
    "        return score\n",
    "    elif scoring_metric == 'precision':\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        return precision\n",
    "    elif scoring_metric == 'recall':\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        return recall\n",
    "    elif scoring_metric == 'auc':\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data = pd.read_csv('/Users/andrewsimon/Desktop/Dow_dat.csv')\n",
    "\n",
    "embeddings = get_bert_embeddings(main_data, 'Report', 'Level')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_downstream(embeddings, main_data['Level'], 0.2, 42)\n",
    "\n",
    "accuracy = logistic_regression_model(X_train, X_test, y_train, y_test, scoring_metric='accuracy')\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_method(data, features, labels, test_size=0.2, random_state=42):\n",
    "    \n",
    "    bow_embeddings = get_BoW_embeddings(data, features, labels)\n",
    "    tf_idf_embeddings = get_tfidf_embeddings(data,features,labels)\n",
    "    bert_embeddings = get_bert_embeddings(data, features, labels)\n",
    "    word2vec_embeddings = get_Word2Vec_embeddings(data,features)\n",
    "    elmo_embeddings = get_elmo_embeddings(data,features)\n",
    "    fasttext_embeddings = get_fasttext_embeddings(data,features, labels)\n",
    "    glove_embeddings = get_glove_embeddings(data,features,labels)\n",
    "\n",
    "    X_train_bow, X_test_bow, y_train, y_test = train_test_split_downstream(bow_embeddings, data[labels], test_size=test_size, random_state=random_state)\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split_downstream(tf_idf_embeddings, data[labels], test_size=test_size, random_state=random_state)\n",
    "    X_train_bert, X_test_bert, y_train, y_test = train_test_split_downstream(bert_embeddings, data[labels], test_size=test_size, random_state=random_state)\n",
    "    X_train_w2v, X_test_w2v, y_train, y_test = train_test_split_downstream(word2vec_embeddings, data[labels], test_size=test_size, random_state=random_state)\n",
    "    X_train_elmo, X_test_elmo, y_train, y_test = train_test_split_downstream(elmo_embeddings, data[labels], test_size=test_size, random_state=random_state)\n",
    "    X_train_fasttext, X_test_fasttext, y_train, y_test = train_test_split_downstream(fasttext_embeddings, data[labels], test_size=test_size, random_state=random_state)\n",
    "    X_train_glove, X_test_glove, y_train, y_test = train_test_split_downstream(glove_embeddings, data[labels], test_size=test_size, random_state=random_state)\n",
    "\n",
    "    bow_embeddings_values = [\n",
    "        random_forest_model(X_train_bow,X_test_bow, y_train, y_test),\n",
    "        decision_tree_model(X_train_bow,X_test_bow, y_train, y_test),\n",
    "        logistic_regression_model(X_train_bow,X_test_bow, y_train, y_test),\n",
    "        svm_model(X_train_bow,X_test_bow, y_train, y_test)\n",
    "    ]\n",
    "\n",
    "    tf_idf_embeddings_values = [\n",
    "        random_forest_model(X_train_tfidf,X_test_tfidf, y_train, y_test),\n",
    "        decision_tree_model(X_train_tfidf,X_test_tfidf, y_train, y_test),\n",
    "        logistic_regression_model(X_train_tfidf,X_test_tfidf, y_train, y_test),\n",
    "        svm_model(X_train_tfidf,X_test_tfidf, y_train, y_test)\n",
    "    ]\n",
    "\n",
    "    bert_embeddings_values = [\n",
    "        random_forest_model(X_train_bert,X_test_bert, y_train, y_test),\n",
    "        decision_tree_model(X_train_bert,X_test_bert, y_train, y_test),\n",
    "        logistic_regression_model(X_train_bert,X_test_bert, y_train, y_test),\n",
    "        svm_model(X_train_bert,X_test_bert, y_train, y_test)\n",
    "    ]\n",
    "\n",
    "    w2v_embeddings_values = [\n",
    "        random_forest_model(X_train_w2v,X_test_w2v, y_train, y_test),\n",
    "        decision_tree_model(X_train_w2v,X_test_w2v, y_train, y_test),\n",
    "        logistic_regression_model(X_train_w2v,X_test_w2v, y_train, y_test),\n",
    "        svm_model(X_train_w2v,X_test_w2v, y_train, y_test)\n",
    "    ]\n",
    "\n",
    "    elmo_embeddings_values = [\n",
    "        random_forest_model(X_train_elmo ,X_test_elmo, y_train, y_test),\n",
    "        decision_tree_model(X_train_elmo ,X_test_elmo, y_train, y_test),\n",
    "        logistic_regression_model(X_train_elmo ,X_test_elmo, y_train, y_test),\n",
    "        svm_model(X_train_elmo ,X_test_elmo, y_train, y_test)\n",
    "    ]\n",
    "\n",
    "    fasttext_embeddings_values = [\n",
    "        random_forest_model(X_train_fasttext ,X_test_fasttext, y_train, y_test),\n",
    "        decision_tree_model(X_train_fasttext ,X_test_fasttext, y_train, y_test),\n",
    "        logistic_regression_model(X_train_fasttext ,X_test_fasttext, y_train, y_test),\n",
    "        svm_model(X_train_fasttext ,X_test_fasttext, y_train, y_test)\n",
    "    ]\n",
    "\n",
    "    glove_embeddings_values = [\n",
    "        random_forest_model(X_train_glove ,X_test_glove, y_train, y_test),\n",
    "        decision_tree_model(X_train_glove ,X_test_glove, y_train, y_test),\n",
    "        logistic_regression_model(X_train_glove ,X_test_glove, y_train, y_test),\n",
    "        svm_model(X_train_glove ,X_test_glove, y_train, y_test)\n",
    "    ]\n",
    "\n",
    "    score_pd = {'Bag of Words': bow_embeddings_values, 'tf idf': tf_idf_embeddings_values, 'BERT': bert_embeddings_values,\n",
    "                'Word2Vec': w2v_embeddings_values, 'ELMo': elmo_embeddings_values, 'FastText': fasttext_embeddings_values,\n",
    "                'GLoVE': glove_embeddings_values}\n",
    "    return pd.DataFrame(data=score_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'read_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewsimon/nlpmaps/nlpmaps/test_selection.ipynb Cell 18\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/nlpmaps/test_selection.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main_data \u001b[39m=\u001b[39m main_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39m\u001b[39m/Users/andrewsimon/Desktop/Dow_dat.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/nlpmaps/test_selection.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pd \u001b[39m=\u001b[39m find_optimal_method(main_data, \u001b[39m'\u001b[39m\u001b[39mReport\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLevel\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'read_csv'"
     ]
    }
   ],
   "source": [
    "main_data = main_data = pd.read_csv('/Users/andrewsimon/Desktop/Dow_dat.csv')\n",
    "pd = find_optimal_method(main_data, 'Report', 'Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "progress-bar: 100%|██████████| 1000/1000 [00:00<00:00, 1403.21it/s]\n",
      "1000it [00:00, 568487.94it/s]\n",
      "/Users/andrewsimon/Library/Python/3.8/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "1000it [00:01, 570.79it/s]\n",
      "2023-06-07 10:57:39.816127: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '#3', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '&,', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\",', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'A)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'B)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2\"', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2\"', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2\"', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2\"', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '5!', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from \"'3\", returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '[6', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"a', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"a', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '(\"', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"a', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '$$', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'T>', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from ').', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'a)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'b)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'c)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'a)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '(D', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"a', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'a)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'b)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'I\"', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from ').', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from ').', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"O', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '½*', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '7,', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '**', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from \"X'\", returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from \"X'\", returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from \"n'\", returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"M', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '**', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '**', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '**', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2*', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'A)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'B)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '4)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '(A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '$6', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '1)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '3)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '4)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '5)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '1)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '3)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '4)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '5)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '6)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '7)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '0,', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2\"', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '8,', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '8,', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '2)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '3)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '4)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '5)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '6)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '7)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '8)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '9,', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '??', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '3\"', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from ':.', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"a', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '8)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '\"A', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'a)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from 'a)', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '$5', returning origin vector\n",
      "WARNING:gensim.models.fasttext:could not extract any ngrams from '7,', returning origin vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n",
      "Numer of samples with no words found: 0 / 1000\n"
     ]
    }
   ],
   "source": [
    "main_data = pd.read_csv('/Users/andrewsimon/Desktop/IMDB_Dataset.csv')\n",
    "main_data['sentiment'] = main_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "main_data = main_data.sample(n=1000)\n",
    "main_data = main_data.reset_index()\n",
    "\n",
    "results = find_optimal_method(main_data, 'review', 'sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bag of Words</th>\n",
       "      <th>tf idf</th>\n",
       "      <th>BERT</th>\n",
       "      <th>Word2Vec</th>\n",
       "      <th>ELMo</th>\n",
       "      <th>FastText</th>\n",
       "      <th>GLoVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bag of Words    tf idf      BERT  Word2Vec      ELMo  FastText     GLoVE\n",
       "0      0.800000  0.716667  0.766667  0.683333  0.750000  0.750000  0.750000\n",
       "1      0.766667  0.733333  0.550000  0.583333  0.633333  0.466667  0.550000\n",
       "2      0.766667  0.683333  0.733333  0.666667  0.683333  0.450000  0.533333\n",
       "3      0.550000  0.616667  0.700000  0.533333  0.450000  0.450000  0.266667"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bag of Words</th>\n",
       "      <th>tf idf</th>\n",
       "      <th>BERT</th>\n",
       "      <th>Word2Vec</th>\n",
       "      <th>ELMo</th>\n",
       "      <th>FastText</th>\n",
       "      <th>GLoVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.670</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bag of Words  tf idf   BERT  Word2Vec   ELMo  FastText  GLoVE\n",
       "0         0.810   0.805  0.665     0.750  0.745     0.755  0.710\n",
       "1         0.670   0.600  0.595     0.560  0.650     0.585  0.560\n",
       "2         0.825   0.830  0.815     0.805  0.815     0.575  0.710\n",
       "3         0.725   0.825  0.745     0.780  0.805     0.670  0.645"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tfidf = tf_idf.generate_tfidf_embeddings(main_data, 'Report', 'Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = embeddings_tfidf.drop(columns='Level').values\n",
    "labels = embeddings_tfidf['Level'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeddings,\n",
    "                                                    main_data['Level'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1100).fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "score = rf.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_BoW = bag_of_words.generate_bow_embeddings(main_data, 'Report', \"Level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = embeddings_BoW.drop(columns='Level').values\n",
    "labels=embeddings_BoW[\"Level\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_array,\n",
    "                                                    labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1100).fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "score = rf.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embeddings = fasttext.fasttext_embedding(main_data, 'Report', 'Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00462546, -0.00377681,  0.00835782, ...,  0.08570157,\n",
       "         0.00314991, -0.00139768],\n",
       "       [ 0.00573479, -0.01565916,  0.00055071, ...,  0.15088515,\n",
       "        -0.00016365, -0.01036716],\n",
       "       [ 0.00575128, -0.00947065,  0.00352919, ...,  0.09792969,\n",
       "        -0.0005991 , -0.00288053],\n",
       "       ...,\n",
       "       [ 0.00498452, -0.02765043, -0.00959891, ...,  0.11969245,\n",
       "        -0.00538165, -0.00801392],\n",
       "       [ 0.00151075, -0.01813442,  0.01682064, ...,  0.08681662,\n",
       "         0.01577524, -0.01038384],\n",
       "       [-0.00617472, -0.02306897,  0.01692365, ...,  0.11087851,\n",
       "        -0.00601448,  0.00964224]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fasttext_embeddings,\n",
    "                                                    main_data['Level'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1100).fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "score = rf.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings = BERT_updated.generate_bert_embeddings(main_data, 'Report', 'Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36320272, -0.32057866, -0.50059915, ..., -0.16303292,\n",
       "        -0.17720942, -0.03341506],\n",
       "       [ 0.05917213, -0.30518496, -0.52072686, ...,  0.22519325,\n",
       "         0.5077894 ,  0.2868259 ],\n",
       "       [-0.24662916, -0.2357285 , -0.49209008, ..., -0.063917  ,\n",
       "        -0.2540468 ,  0.05076841],\n",
       "       ...,\n",
       "       [-0.34315735, -0.39122057, -0.69828576, ...,  0.40737885,\n",
       "         0.36793244,  0.00545898],\n",
       "       [-0.41907865, -0.6085989 , -0.62236917, ..., -0.18170431,\n",
       "        -0.05564305, -0.29402852],\n",
       "       [-0.342932  , -0.18032494, -0.31473652, ...,  0.3635915 ,\n",
       "         0.29547995, -0.00772604]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bert_embeddings,\n",
    "                                                    main_data['Level'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1100).fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "score = rf.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n",
      "Numer of samples with no words found: 0 / 300\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = glove.glove_embedding(main_data, 'Report', 'Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(glove_embeddings,\n",
    "                                                    main_data['Level'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6833333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1100).fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "score = rf.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
