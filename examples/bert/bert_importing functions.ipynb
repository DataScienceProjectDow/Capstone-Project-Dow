{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5231e1f",
   "metadata": {},
   "source": [
    "### About BERT\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers. \n",
    "\n",
    "### Classify text with BERT\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- Load the IMDB dataset\n",
    "- Load a BERT model from TensorFlow Hub\n",
    "- Build model by combining BERT with a classifier\n",
    "- Train model, fine-tuning BERT as part of that\n",
    "- Save model and use it to classify sentences\n",
    "\n",
    "### Sentiment analysis\n",
    "\n",
    "This notebook trains a sentiment analysis model to classify movie reviews as *positive* or *negative*, based on the text of the review.\n",
    "\n",
    "We'll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934fee81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlpmaps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16800\\764552328.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/meenal/Capstone-Project-Dow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# add the parent directory to the sys.path list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnlpmaps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlpmaps'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('/home/meenal/nlpmaps'))  # add the parent directory to the sys.path list\n",
    "from nlpmaps.bert import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52bf1d",
   "metadata": {},
   "source": [
    "The provided code snippet is used to prepare the IMDB dataset for sentiment analysis of movie reviews. The text_dataset_from_directory utility is used to create labeled tf.data.Dataset objects for the training, validation, and testing datasets. The batch size is set to 32 and a random seed of 42 is used for consistency in results. The validation_split argument is used to create a validation set from 20% of the training data, and the remaining training data is cached and prefetched using AUTOTUNE for improved performance during training. Similarly, the validation and testing datasets are also cached and prefetched using AUTOTUNE. The class names for the sentiment labels are retrieved from the training dataset. Overall, this code sets up the IMDB dataset for sentiment analysis and optimizes data loading and preprocessing performance using AUTOTUNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "dataset_dir = download_and_extract_dataset(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001b19b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "validation_split=0.2\n",
    "\n",
    "train_ds, val_ds, test_ds, class_names = load_imdb_dataset(batch_size=batch_size, validation_split=0.2, seed=seed)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "006f0597",
   "metadata": {},
   "source": [
    "In this code, we are looping over the first batch of the training dataset using train_ds.take(1). Within this loop, we are then looping over the first 3 reviews in the batch using range(3). For each review, we print out the text using text_batch.numpy()[i], and then get the corresponding label for the review using label_batch.numpy()[i]. Finally, we print out the label value and its corresponding class name using class_names[label].\n",
    "\n",
    "Note that we are only printing out the first batch of reviews here for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpmaps.bert import print_first_batch\n",
    "print_first_batch(train_ds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21402bbf",
   "metadata": {},
   "source": [
    "### Loading models from TensorFlow Hub\n",
    "\n",
    "Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n",
    "\n",
    "  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n",
    "  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
    "  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n",
    "  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n",
    "  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n",
    "  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n",
    "\n",
    "We have the option to choose which BERT model we want to load from TensorFlow Hub and fine-tune for our sentiment analysis task. We decided to go with Small BERTs since they have fewer and/or smaller Transformer blocks, making them faster and more memory-efficient to train and deploy. This is particularly useful since our current model is taking 45 minutes per epoch to train, and we have limited computational resources. Additionally, Small BERTs still maintain a good level of performance on NLP tasks like sentiment analysis. The other BERT models available have different trade-offs between speed, size, and quality, and we can choose one of them if we want even better accuracy or have more resources available. All these models can be loaded from their corresponding TensorFlow Hub URLs using the code block provided.\n",
    "\n",
    "You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub.\n",
    "\n",
    "The first variable bert_model_name is a string which takes one of the given BERT models as input. The second variable map_name_to_handle is a dictionary which maps each BERT model name to its corresponding TensorFlow Hub URL. The URLs are used to load the pre-trained BERT models using TensorFlow Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name= 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
    "tfhub_handle_encoder, tfhub_handle_preprocess = select_bert_model(bert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11da09",
   "metadata": {},
   "source": [
    "### The preprocessing model\n",
    "\n",
    "We need to transform text inputs into numeric token ids and arrange them in Tensors before inputting them to BERT. Luckily, TensorFlow Hub provides a matching preprocessing model for each BERT model we choose, which implements this transformation using TF ops from the TF.text library. This means we don't need to run pure Python code outside of our TensorFlow model to preprocess text.\n",
    "\n",
    "It's important to use the preprocessing model that is referenced in the documentation of the BERT model we choose, which we can find at the URL printed above. If we choose a BERT model from the drop-down above, the preprocessing model will be selected automatically.\n",
    "\n",
    "Note: To load the preprocessing model into our fine-tuned model, we'll use the hub.KerasLayer API. This is the preferred way to load a TF2-style SavedModel from TF Hub into a Keras model.\n",
    "\n",
    "We can try the preprocessing model on some text and observe the output.\n",
    "\n",
    "We can see that we have obtained the 3 outputs from the preprocessing that a BERT model would use (input_words_id, input_mask, and input_type_ids).\n",
    "\n",
    "It is important to note that the input is truncated to 128 tokens, although the number of tokens can be customized as required.\n",
    "\n",
    "Also, the input_type_ids only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.\n",
    "\n",
    "Since this text preprocessor is a TensorFlow model, we can include it directly in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ff40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = preprocess_text(tfhub_handle_preprocess, text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d7ce6",
   "metadata": {},
   "source": [
    "### Using the BERT model\n",
    "\n",
    "Now that we have our preprocessed inputs, let's see how we can use the BERT model. We will load the BERT model from TF Hub and examine its outputs.\n",
    "\n",
    "First, we will load the BERT model from the TF Hub using hub.KerasLayer(). Then, we will pass our preprocessed inputs to the BERT model to get its outputs. The outputs from the BERT model are embeddings for each token in the input sequence, along with a pooled embedding for the entire sequence.\n",
    "\n",
    "The embeddings for each token can be used for various NLP tasks such as classification, question answering, etc. The pooled embedding is a condensed representation of the entire input sequence that can be used for tasks such as sentence similarity, clustering, etc.\n",
    "\n",
    "Let's take a closer look at the BERT model outputs and see how we can use them in our downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87765f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outputs = get_bert_outputs(text_preprocessed, tfhub_handle_encoder)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape: {bert_outputs[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values: {bert_outputs[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape: {bert_outputs[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values: {bert_outputs[\"sequence_output\"][0, :12]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3774c",
   "metadata": {},
   "source": [
    "As we have loaded the BERT model and preprocessed our text, we can see the outputs of the BERT model. The BERT model returns a dictionary with three keys: pooled_output, sequence_output, and encoder_outputs.\n",
    "\n",
    "- pooled_output is the embedding representation of the entire sequence. The shape of this tensor is [batch_size, H], where H is the hidden size of the BERT model.\n",
    "\n",
    "\n",
    "- sequence_output is the contextual embedding of each token in the input sequence. The shape of this tensor is [batch_size, seq_length, H], where seq_length is the maximum sequence length of the input text.\n",
    "\n",
    "\n",
    "- encoder_outputs are the intermediate activations of the L transformer blocks, where outputs[\"encoder_outputs\"][i] is a tensor of shape [batch_size, seq_length, 1024] with the outputs of the i-th Transformer block, for 0 <= i < L. The last value of the list is equal to sequence_output.\n",
    "For fine-tuning our BERT model, we will use the pooled_output tensor, which represents the entire sequence as an embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc31c2",
   "metadata": {},
   "source": [
    "### Define your model\n",
    "\n",
    "In this step, we are going to define our own fine-tuned model. We will use the text preprocessing model and the BERT model that we loaded in the previous steps, along with a Dense layer and a Dropout layer.\n",
    "\n",
    "It is important to mention that the preprocessing model we used in the previous step will take care of the inputs and outputs of the BERT model. Therefore, we don't need to worry about it much, but if you need more information about the base model, you can refer to its URL for documentation.\n",
    "\n",
    "Our model will be very simple but efficient. We will combine the output of the BERT model, which is the pooled_output array, with a Dense layer and a Dropout layer to avoid overfitting.\n",
    "\n",
    "We will run the model using the output of the preprocessing model to check if it's working correctly.\n",
    "\n",
    "First, we will load the build_classifier_model function, which builds the classification model using the BERT encoder.\n",
    "\n",
    "Then, we will define a sample text and pass it through the preprocessing model.\n",
    "\n",
    "Finally, we will pass the preprocessed text to the classification model and check if it returns a valid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model(tfhub_handle_encoder, tfhub_handle_preprocess)\n",
    "# obtain the raw output from the BERT model\n",
    "bert_raw_result = classifier_model(tf.constant(text_test)) \n",
    "# apply sigmoid function to the output to obtain the probabilities for the classes\n",
    "print(tf.sigmoid(bert_raw_result)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98964eea",
   "metadata": {},
   "source": [
    "Let's take a look at the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be116eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94c9f6",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Now that we have all the necessary components such as the preprocessing module, BERT encoder, data, and classifier, we can train the model. Training the model involves the following steps:\n",
    "\n",
    "- Defining the optimizer, loss function, and evaluation metrics.\n",
    "- Compiling the model with the optimizer and loss function.\n",
    "- Fitting the model to the training data with a specified batch size and number of epochs.\n",
    "\n",
    "The goal of training the model is to update the weights of the neural network such that the loss function is minimized and the evaluation metrics are maximized. Once the model has been trained, we can evaluate its performance on the validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a501ed",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), we'll use `losses.BinaryCrossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and assign the results to variables\n",
    "binary_loss, metrics = build_loss_and_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59e40a",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Now that we have our model defined and the optimizer, loss function, and learning rate schedule selected, we can proceed to fine-tune the BERT model.\n",
    "\n",
    "We will use the model.fit() function in TensorFlow to train the model. The training process will involve passing batches of the preprocessed movie reviews to the model and updating the weights of the model based on the loss calculated by the loss function.\n",
    "\n",
    "During the training process, we will track the loss and accuracy of the model on both the training and validation sets. This will allow us to monitor the progress of the training and identify any overfitting or underfitting of the model.\n",
    "\n",
    "We will use early stopping to prevent overfitting. Early stopping will monitor the validation loss and stop the training process if the loss stops improving for a certain number of epochs.\n",
    "\n",
    "Once the model is trained, we will evaluate its performance on the test set to get an estimate of its real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a874770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "init_lr = 3e-5\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = create_optimizer(init_lr, num_train_steps, num_warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38adcc",
   "metadata": {},
   "source": [
    "### Loading the BERT model and training\n",
    "\n",
    "Using the `classifier_model` we created earlier, you can compile the model with the loss, metric and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8179bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=binary_loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1392c3",
   "metadata": {},
   "source": [
    "Note: training time will vary depending on the complexity of the BERT model you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9952f821",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2c8cc",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e2952",
   "metadata": {},
   "source": [
    "### Plot the accuracy and loss over time\n",
    "\n",
    "Based on the `History` object returned by `model.fit()`. We can plot the training and validation loss for comparison, as well as the training and validation accuracy. In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ca068",
   "metadata": {},
   "source": [
    "### Export for inference\n",
    "\n",
    "Now just save your fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0115eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'imdb'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "save_model(classifier_model, dataset_name, saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d6ec7",
   "metadata": {},
   "source": [
    "Here you can test your model on any sentence you want, just add to the examples variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646984c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    'this is such a spectacular movie!', \n",
    "    'The movie was great!',\n",
    "    'The movie was meh.',\n",
    "    'The movie was okish.',\n",
    "    'The movie was terrible...'\n",
    "]\n",
    "\n",
    "results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the model:')\n",
    "print_my_examples(examples, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd26f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222bd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0f444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
