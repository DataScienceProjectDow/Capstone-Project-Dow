{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained GloVe vectors from the website\n",
    "URL = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "FILE_NAME = 'glove.6B.zip'\n",
    "urllib.request.urlretrieve(URL, FILE_NAME)\n",
    "\n",
    "with zipfile.ZipFile(FILE_NAME, 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# Transfer pre-trained GloVe vectors into word2vec format\n",
    "GLOVE_FILE = 'glove.6B.100d.txt'\n",
    "WORD2VEC_FILE = GLOVE_FILE + '.word2vec'\n",
    "glove2word2vec(GLOVE_FILE, WORD2VEC_FILE)\n",
    "\n",
    "# Load pre-trained GloVe vectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(WORD2VEC_FILE, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b271006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df1 = pd.read_excel('../../../Downloads/CHEME DIRECT/NLP Project/PSE Dataset.xlsx', sheet_name='SamePerson Report')\n",
    "df2 = pd.read_excel('../../../Downloads/CHEME DIRECT/NLP Project/PSE Dataset.xlsx', sheet_name='Multiple People Report')\n",
    "df3 = pd.read_excel('../../../Downloads/CHEME DIRECT/NLP Project/PSE Dataset.xlsx', sheet_name='Multiple People Less Details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and testing dataset\n",
    "dev_1, test_1 = sklearn.model_selection.train_test_split(df1, test_size=0.2, random_state=26)\n",
    "dev_2, test_2 = sklearn.model_selection.train_test_split(df2, test_size=0.2, random_state=26)\n",
    "dev_3, test_3 = sklearn.model_selection.train_test_split(df3, test_size=0.2, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87867133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text and label from each dataset\n",
    "dev_1_text = dev_1['Report'].values.reshape(-1,1)\n",
    "test_1_text = test_1['Report'].values.reshape(-1,1)\n",
    "\n",
    "dev_1_label = dev_1['Level'].values.reshape(-1,1)\n",
    "test_1_label = test_1['Level'].values.reshape(-1,1)\n",
    "\n",
    "dev_2_text = dev_2['Report'].values.reshape(-1,1)\n",
    "test_2_text = test_2['Report'].values.reshape(-1,1)\n",
    "\n",
    "dev_2_label = dev_2['Level'].values.reshape(-1,1)\n",
    "test_2_label = test_2['Level'].values.reshape(-1,1)\n",
    "\n",
    "dev_3_text = dev_3['Report'].values.reshape(-1,1)\n",
    "test_3_text = test_3['Report'].values.reshape(-1,1)\n",
    "\n",
    "dev_3_label = dev_3['Level'].values.reshape(-1,1)\n",
    "test_3_label = test_3['Level'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for tokenizing the text data\n",
    "class Word2VecVectorizer:\n",
    "    def __init__(self, model_vec):\n",
    "        print(\"Loading in word vectors...\")\n",
    "        self.word_vectors = model_vec\n",
    "        print(\"Finished loading in word vectors\")\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"fit data\"\"\"\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"determine the dimensionality of vectors\"\"\"\n",
    "        v_get = self.word_vectors.get_vector('king')\n",
    "        self.D = v_get.shape[0]\n",
    "\n",
    "        x_vec = np.zeros((len(data), self.D))\n",
    "        n_count = 0\n",
    "        emptycount = 0\n",
    "\n",
    "        for sentence in data:\n",
    "            tokens = sentence.split()\n",
    "            vecs = []\n",
    "            m_count = 0\n",
    "            for word in tokens:\n",
    "                try:\n",
    "                    # throws KeyError if word not found\n",
    "                    vec = self.word_vectors.get_vector(word)\n",
    "                    vecs.append(vec)\n",
    "                    m_count += 1\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                x_vec[n_count] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount += 1\n",
    "            n_count += 1\n",
    "\n",
    "        print(\"Numer of samples with no words found: %s / %s\" % (emptycount,\n",
    "        \tlen(data)))\n",
    "        return x_vec\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"transform the strings to vectors\"\"\"\n",
    "        self.fit(data)\n",
    "\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert texts and labels into format of list\n",
    "def to_list(data):\n",
    "    list_of_list = data.tolist()\n",
    "    new_data = [item for sublist in list_of_list for item in sublist]\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists\n",
    "dev_1_text_str = to_list(dev_1_text)\n",
    "dev_1_label_str = to_list(dev_1_label)\n",
    "\n",
    "dev_2_text_str = to_list(dev_2_text)\n",
    "dev_2_label_str = to_list(dev_2_label)\n",
    "\n",
    "dev_3_text_str = to_list(dev_3_text)\n",
    "dev_3_label_str = to_list(dev_3_label)\n",
    "\n",
    "test_1_text_str = to_list(test_1_text)\n",
    "test_1_label_str = to_list(test_1_label)\n",
    "\n",
    "test_2_text_str = to_list(test_2_text)\n",
    "test_2_label_str = to_list(test_2_label)\n",
    "\n",
    "test_3_text_str = to_list(test_3_text)\n",
    "test_3_label_str = to_list(test_3_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate three sets together\n",
    "dev_text = dev_1_text_str + dev_2_text_str + dev_3_text_str\n",
    "test_text = test_1_text_str + test_2_text_str + test_3_text_str\n",
    "\n",
    "dev_label = dev_1_label_str + dev_2_label_str + dev_3_label_str\n",
    "test_label = test_1_label_str + test_2_label_str + test_3_label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cd949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set a word vectorizer\n",
    "vectorizer = Word2VecVectorizer(glove_model)\n",
    "\n",
    "# Get the sentence embeddings for the train dataset\n",
    "dev_x = vectorizer.fit_transform(dev_text)\n",
    "dev_y = dev_label\n",
    "\n",
    "# Get the sentence embeddings for the test dataset\n",
    "test_x = vectorizer.transform(test_text)\n",
    "test_y = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36978b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set to have a validation set\n",
    "train_x, val_x, train_y, val_y = sklearn.model_selection.train_test_split(\n",
    "    dev_x, dev_y, test_size=0.2, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff76f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization using Optuna\n",
    "def train_evaluate_hyperparameters(n_estimators, max_depth, min_weight_fraction_leaf):\n",
    "    # create model\n",
    "    model = sklearn.ensemble.RandomForestClassifier(\n",
    "        n_estimators=n_estimators, max_depth=max_depth, min_weight_fraction_leaf=min_weight_fraction_leaf)\n",
    "    # train the model on the training set\n",
    "    model.fit(train_x, train_y)\n",
    "    # evaluate the model on the validation set\n",
    "    score = model.score(val_x, val_y)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # define hyperparameter space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 5000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 600)\n",
    "    min_weight_fraction_leaf = trial.suggest_uniform(\n",
    "        'min_weight_fraction_leaf', 0.0, 0.5)\n",
    "    \n",
    "    # get the score for the hyperparameters chosen\n",
    "    score = train_evaluate_hyperparameters(n_estimators, max_depth, min_weight_fraction_leaf)\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "print('Best params: ', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "model = sklearn.ensemble.RandomForestClassifier(\n",
    "    n_estimators=4974, min_weight_fraction_leaf=0.008440029412940517, max_depth=18)\n",
    "\n",
    "model.fit(dev_x, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108758c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy report\n",
    "print(\"model accuracy:\", model.score(test_x, test_y))\n",
    "\n",
    "predict_y = model.predict(test_x)\n",
    "\n",
    "print(sklearn.metrics.classification_report(test_y, predict_y, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix\n",
    "sklearn.metrics.ConfusionMatrixDisplay.from_estimator(model, test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
