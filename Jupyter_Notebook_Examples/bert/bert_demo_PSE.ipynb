{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load dataset\n",
    "df1 = pd.read_excel(r'C:\\Users\\stlp\\Desktop\\PSE Dataset.xlsx', sheet_name='SamePerson Report')\n",
    "df2 = pd.read_excel(r'C:\\Users\\stlp\\Desktop\\PSE Dataset.xlsx', sheet_name='Multiple People Report')\n",
    "df3 = pd.read_excel(r'C:\\Users\\stlp\\Desktop\\PSE Dataset.xlsx', sheet_name='Multiple People Less Details')\n",
    "\n",
    "train_1, test_1 = sklearn.model_selection.train_test_split(df1, test_size=0.2, random_state=206)\n",
    "train_2, test_2 = sklearn.model_selection.train_test_split(df2, test_size=0.2, random_state=206)\n",
    "train_3, test_3 = sklearn.model_selection.train_test_split(df3, test_size=0.2, random_state=206)\n",
    "\n",
    "train_1_text = train_1['Report'].values.reshape(-1,1)\n",
    "test_1_text = test_1['Report'].values.reshape(-1,1)\n",
    "\n",
    "train_1_label = train_1['Level'].values.reshape(-1,1)\n",
    "test_1_label = test_1['Level'].values.reshape(-1,1)\n",
    "\n",
    "train_2_text = train_2['Report'].values.reshape(-1,1)\n",
    "test_2_text = test_2['Report'].values.reshape(-1,1)\n",
    "\n",
    "train_2_label = train_2['Level'].values.reshape(-1,1)\n",
    "test_2_label = test_2['Level'].values.reshape(-1,1)\n",
    "\n",
    "train_3_text = train_3['Report'].values.reshape(-1,1)\n",
    "test_3_text = test_3['Report'].values.reshape(-1,1)\n",
    "\n",
    "train_3_label = train_3['Level'].values.reshape(-1,1)\n",
    "test_3_label = test_3['Level'].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "# Define functions for tokenizing the text data using BERT tokenizer\n",
    "def tokenize_text(text):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in text:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "train_1_text_list = train_1_text.tolist()\n",
    "train_1_text_str = [item for sublist in train_1_text_list for item in sublist]\n",
    "\n",
    "test_1_text_list = test_1_text.tolist()\n",
    "test_1_text_str = [item for sublist in test_1_text_list for item in sublist]\n",
    "\n",
    "train_1_label_list = train_1_label.tolist()\n",
    "train_1_label_str = [item for sublist in train_1_label_list for item in sublist]\n",
    "\n",
    "test_1_label_list = test_1_label.tolist()\n",
    "test_1_label_str = [item for sublist in test_1_label_list for item in sublist]\n",
    "\n",
    "\n",
    "# Tokenize the train dataset\n",
    "train_input_ids, train_attention_masks = tokenize_text(train_1_text_str)\n",
    "train_labels = torch.tensor(train_1_label_str)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "test_input_ids, test_attention_masks = tokenize_text(test_1_text_str)\n",
    "test_labels = torch.tensor(test_1_label_str)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move data to the device\n",
    "train_input_ids = train_input_ids.to(device)\n",
    "train_attention_masks = train_attention_masks.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "\n",
    "test_input_ids = test_input_ids.to(device)\n",
    "test_attention_masks = test_attention_masks.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Forward pass through BERT model\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(train_input_ids, attention_mask=train_attention_masks)\n",
    "    test_outputs = model(test_input_ids, attention_mask=test_attention_masks)\n",
    "\n",
    "train_features = train_outputs.pooler_output\n",
    "test_features = test_outputs.pooler_output\n",
    "\n",
    "# Convert features to numpy arrays\n",
    "train_x = train_features.cpu().numpy()\n",
    "test_x = test_features.cpu().numpy()\n",
    "train_y = train_labels.cpu().numpy()\n",
    "test_y = test_labels.cpu().numpy()\n",
    "classifiers = {\n",
    "    \"Random Forest\": {\n",
    "        \"model\": sklearn.ensemble.RandomForestClassifier(),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 4, 6],\n",
    "            \"max_features\": [0.5, 0.75, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(),\n",
    "        \"param_grid\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"kernel\": ['linear', 'rbf'],\n",
    "            \"gamma\": ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "            \"max_depth\": [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    \"Multi-Layer Perceptron\": {\n",
    "        \"model\": MLPClassifier(),\n",
    "        \"param_grid\": {\n",
    "            \"hidden_layer_sizes\": [(50,), (100,), (200,)],\n",
    "            \"activation\": ['relu', 'tanh'],\n",
    "            \"solver\": ['adam', 'sgd'],\n",
    "            \"learning_rate\": ['constant', 'adaptive']\n",
    "        }\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"model\": AdaBoostClassifier(),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.1, 0.01, 0.001]\n",
    "        }\n",
    "    },\n",
    "    \"Bagging\": {\n",
    "        \"model\": BaggingClassifier(),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [10, 50, 100],\n",
    "            \"max_samples\": [0.5, 0.75, 1.0],\n",
    "            \"max_features\": [0.5, 0.75, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"Extra Trees\": {\n",
    "        \"model\": ExtraTreesClassifier(),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 4, 6]\n",
    "        }\n",
    "    }}\n",
    "# Function to run grid search for a classifier\n",
    "def run_grid_search(classifier_name, classifier, param_grid, train_x, train_y):\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_params, best_model\n",
    "\n",
    "# Function to evaluate the model and plot the confusion matrix\n",
    "def evaluate_model(model, test_x, test_y):\n",
    "    pred_y = model.predict(test_x)\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_y, pred_y)\n",
    "    confusion_mat = sklearn.metrics.confusion_matrix(test_y, pred_y)\n",
    "    class_names = np.unique(test_y)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.xticks(np.arange(len(class_names)) + 0.5, class_names)\n",
    "    plt.yticks(np.arange(len(class_names)) + 0.5, class_names)\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, confusion_mat\n",
    "\n",
    "# Run grid search for each classifier\n",
    "results = {}\n",
    "\n",
    "for classifier_name, classifier_info in classifiers.items():\n",
    "    print(f\"Running grid search for {classifier_name}...\")\n",
    "    param_grid = classifier_info[\"param_grid\"]\n",
    "    classifier = classifier_info[\"model\"]\n",
    "    best_params, best_model = run_grid_search(classifier_name, classifier, param_grid, train_x, train_y)\n",
    "    results[classifier_name] = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_model\": best_model\n",
    "    }\n",
    "    print(f\"Best parameters for {classifier_name}: {best_params}\")\n",
    "    print(f\"Best model for {classifier_name}: {best_model}\")\n",
    "    print()\n",
    "    \n",
    "    # Evaluate the best model\n",
    "    accuracy, confusion_mat = evaluate_model(best_model, test_x, test_y)\n",
    "    print(f\"Accuracy for {classifier_name}: {accuracy}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "    print(\"-------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "# Plot confusion matrix for each classifier\n",
    "for classifier_name, result in results.items():\n",
    "    best_model = result[\"best_model\"]\n",
    "    print(f\"Confusion matrix for {classifier_name}:\")\n",
    "    _, _ = evaluate_model(best_model, test_x, test_y)\n",
    "    print(\"-------------------------------------------\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
