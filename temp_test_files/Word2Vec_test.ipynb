{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a juptyer notebook detailing the various methods I used to train a W2v model on the Dow chemical dataset. Results summarized at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "\n",
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>On April 10th at 12:30 PM, there was an incide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>On April 8th at 9:00 PM, a fire broke out in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>On April 6th at 2:45 AM, there was an incident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>On April 5th at 7:15 PM, there was an unplanne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>On April 2nd at 11:30 AM, there was a minor in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>3</td>\n",
       "      <td>At 9:45 AM, a worker reported a small leak of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>5</td>\n",
       "      <td>At 2:30 PM, a worker reported a small spill of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>4</td>\n",
       "      <td>At 6:15 AM, a worker reported an unusual odor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2</td>\n",
       "      <td>At 11:00 AM, a worker reported a small fire in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1</td>\n",
       "      <td>At 4:00 PM, an unplanned event occurred in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Level                                             Report\n",
       "0        2  On April 10th at 12:30 PM, there was an incide...\n",
       "1        3  On April 8th at 9:00 PM, a fire broke out in t...\n",
       "2        4  On April 6th at 2:45 AM, there was an incident...\n",
       "3        1  On April 5th at 7:15 PM, there was an unplanne...\n",
       "4        5  On April 2nd at 11:30 AM, there was a minor in...\n",
       "..     ...                                                ...\n",
       "295      3  At 9:45 AM, a worker reported a small leak of ...\n",
       "296      5  At 2:30 PM, a worker reported a small spill of...\n",
       "297      4  At 6:15 AM, a worker reported an unusual odor ...\n",
       "298      2  At 11:00 AM, a worker reported a small fire in...\n",
       "299      1  At 4:00 PM, an unplanned event occurred in the...\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 1519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data into a pandas dataframe\n",
    "\n",
    "main_data = pd.read_csv('/Users/andrewsimon/Desktop/Dow_dat.csv')\n",
    "main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows per safety level rating:\n",
      "3    66\n",
      "2    62\n",
      "4    62\n",
      "5    62\n",
      "1    48\n",
      "Name: Level, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exploring the distribution of values in safety levels:\n",
    "print(\"Number of rows per safety level rating:\")\n",
    "print(main_data['Level'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the data with the NTLK tokenizer and fitting the data to a Keras Sequential model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer helper function\n",
    "def tokenize(report):\n",
    "    try:\n",
    "        tokens = tokenizer.tokenize(report)\n",
    "        return tokens\n",
    "    except:\n",
    "        return \"NC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer wrapping function\n",
    "def postprocess(data):\n",
    "    data['tokens'] = data['Report'].progress_map(tokenize)\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 300/300 [00:00<00:00, 6221.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data with wrapping functions\n",
    "data = main_data.copy()\n",
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data.tokens),\n",
    "                                                    np.array(data.Level), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [00:00, 1371434.55it/s]\n",
      "60it [00:00, 121927.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Attach Doc2vec labels to each of our tokens\n",
    "def labelizeReports(reports, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(reports)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(TaggedDocument(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "X_train = labelizeReports(X_train, 'TRAIN')\n",
    "X_test = labelizeReports(X_test, 'TEST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 2884335.13it/s]\n",
      "100%|██████████| 240/240 [00:00<00:00, 3471148.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(39106, 90025)"
      ]
     },
     "execution_count": 1526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit train model\n",
    "tweet_w2v = Word2Vec(vector_size=300, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(X_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(X_train)], total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1527,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewsimon/Library/Python/3.8/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Extract word embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word vectors from embeddings\n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += google_vectors[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [00:00, 5753.63it/s]\n",
      "60it [00:00, 5597.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data postprocessing with sklearn, creating unit standard deviation\n",
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in tqdm(map(lambda x: x.words, X_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in tqdm(map(lambda x: x.words, X_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "8/8 - 0s - loss: 0.8034 - accuracy: 0.0042 - val_loss: -2.5347e+00 - val_accuracy: 0.0000e+00 - 315ms/epoch - 39ms/step\n",
      "Epoch 2/9\n",
      "8/8 - 0s - loss: -2.9687e+00 - accuracy: 0.0083 - val_loss: -5.3913e+00 - val_accuracy: 0.0000e+00 - 26ms/epoch - 3ms/step\n",
      "Epoch 3/9\n",
      "8/8 - 0s - loss: -5.5191e+00 - accuracy: 0.0125 - val_loss: -8.0628e+00 - val_accuracy: 0.0167 - 39ms/epoch - 5ms/step\n",
      "Epoch 4/9\n",
      "8/8 - 0s - loss: -7.9084e+00 - accuracy: 0.0125 - val_loss: -1.0688e+01 - val_accuracy: 0.0333 - 45ms/epoch - 6ms/step\n",
      "Epoch 5/9\n",
      "8/8 - 0s - loss: -1.0437e+01 - accuracy: 0.0208 - val_loss: -1.3523e+01 - val_accuracy: 0.0333 - 24ms/epoch - 3ms/step\n",
      "Epoch 6/9\n",
      "8/8 - 0s - loss: -1.3086e+01 - accuracy: 0.0250 - val_loss: -1.6275e+01 - val_accuracy: 0.0333 - 19ms/epoch - 2ms/step\n",
      "Epoch 7/9\n",
      "8/8 - 0s - loss: -1.5735e+01 - accuracy: 0.0333 - val_loss: -1.9181e+01 - val_accuracy: 0.0333 - 19ms/epoch - 2ms/step\n",
      "Epoch 8/9\n",
      "8/8 - 0s - loss: -1.8526e+01 - accuracy: 0.0458 - val_loss: -2.2269e+01 - val_accuracy: 0.0500 - 19ms/epoch - 2ms/step\n",
      "Epoch 9/9\n",
      "8/8 - 0s - loss: -2.1507e+01 - accuracy: 0.0458 - val_loss: -2.5465e+01 - val_accuracy: 0.0500 - 19ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x49935f910>"
      ]
     },
     "execution_count": 1530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(32, activation='relu', input_dim=300))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2, validation_data = (test_vecs_w2v, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is performing worse than a random guess at this point. The tokenization leaves a large amount of stop words in the corpus, which can be solved using Gensim.preprocess."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gensim tokenization and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Report</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>On April 10th at 12:30 PM, there was an incide...</td>\n",
       "      <td>[on, april, th, at, pm, there, was, an, incide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>On April 8th at 9:00 PM, a fire broke out in t...</td>\n",
       "      <td>[on, april, th, at, pm, fire, broke, out, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>On April 6th at 2:45 AM, there was an incident...</td>\n",
       "      <td>[on, april, th, at, am, there, was, an, incide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>On April 5th at 7:15 PM, there was an unplanne...</td>\n",
       "      <td>[on, april, th, at, pm, there, was, an, unplan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>On April 2nd at 11:30 AM, there was a minor in...</td>\n",
       "      <td>[on, april, nd, at, am, there, was, minor, inc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Level                                             Report  \\\n",
       "0      2  On April 10th at 12:30 PM, there was an incide...   \n",
       "1      3  On April 8th at 9:00 PM, a fire broke out in t...   \n",
       "2      4  On April 6th at 2:45 AM, there was an incident...   \n",
       "3      1  On April 5th at 7:15 PM, there was an unplanne...   \n",
       "4      5  On April 2nd at 11:30 AM, there was a minor in...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [on, april, th, at, pm, there, was, an, incide...  \n",
       "1  [on, april, th, at, pm, fire, broke, out, in, ...  \n",
       "2  [on, april, th, at, am, there, was, an, incide...  \n",
       "3  [on, april, th, at, pm, there, was, an, unplan...  \n",
       "4  [on, april, nd, at, am, there, was, minor, inc...  "
      ]
     },
     "execution_count": 1531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gensim = main_data.copy()\n",
    "\n",
    "data_gensim['tokens'] = data_gensim['Report'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "data_gensim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data_gensim.tokens),\n",
    "                                                    np.array(data_gensim.Level), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [00:00, 354573.08it/s]\n",
      "60it [00:00, 1052963.35it/s]\n",
      "100%|██████████| 240/240 [00:00<00:00, 2728002.60it/s]\n",
      "100%|██████████| 240/240 [00:00<00:00, 3756093.13it/s]\n",
      "/Users/andrewsimon/Library/Python/3.8/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 300 into shape (1,200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Data postprocessing with sklearn, creating unit standard deviation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m scale\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_vecs_w2v \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([buildWordVector(z, \u001b[39m200\u001b[39m) \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m tqdm(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mwords, X_train))])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_vecs_w2v \u001b[39m=\u001b[39m scale(train_vecs_w2v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m test_vecs_w2v \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([buildWordVector(z, \u001b[39m200\u001b[39m) \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m tqdm(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mwords, X_test))])\n",
      "\u001b[1;32m/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Data postprocessing with sklearn, creating unit standard deviation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m scale\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_vecs_w2v \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([buildWordVector(z, \u001b[39m200\u001b[39;49m) \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m tqdm(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mwords, X_train))])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_vecs_w2v \u001b[39m=\u001b[39m scale(train_vecs_w2v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m test_vecs_w2v \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([buildWordVector(z, \u001b[39m200\u001b[39m) \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m tqdm(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mwords, X_test))])\n",
      "\u001b[1;32m/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb Cell 20\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokens:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         vec \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m google_vectors[word]\u001b[39m.\u001b[39;49mreshape((\u001b[39m1\u001b[39;49m, size)) \u001b[39m*\u001b[39m tfidf[word]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m: \u001b[39m# handling the case where the token is not\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                      \u001b[39m# in the corpus. useful for testing.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 300 into shape (1,200)"
     ]
    }
   ],
   "source": [
    "X_train = labelizeReports(X_train, 'TRAIN')\n",
    "X_test = labelizeReports(X_test, 'TEST')\n",
    "\n",
    "# Fit train model\n",
    "tweet_w2v = Word2Vec(vector_size=200, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(X_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(X_train)], total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.epochs)\n",
    "\n",
    "# Extract word embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "\n",
    "# Data postprocessing with sklearn, creating unit standard deviation\n",
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, X_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, X_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "# Training model\n",
    "model_gensim = keras.Sequential()\n",
    "model_gensim.add(keras.layers.Dense(32, activation='relu', input_dim=200))\n",
    "model_gensim.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model_gensim.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_gensim.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2, validation_data=(test_vecs_w2v, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim preprocessing boosted our score to 0.17, slightly better but still worse performance than a random guess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Padded Sequence Embeddings as labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the ragged arrays, Keras sequence padding can be used rather than taking a weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 300/300 [00:00<00:00, 6266.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Since tokenization did better for our model, use that for preprocessing\n",
    "data_padded = main_data.copy()\n",
    "data_padded = postprocess(data_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding vectors to an array from trained model, if out of vocabulary (oov) error occurs just add empty numpy array of all zeros\n",
    "\n",
    "for i in range(len(data_padded['tokens'])):\n",
    "    sub_list = []\n",
    "    for j in data_padded['tokens'][i]:\n",
    "        try:\n",
    "            sub_list.append(tweet_w2v.wv.get_vector(j))\n",
    "        except KeyError:\n",
    "            sub_list.append(np.zeros(200,))\n",
    "    data_padded['tokens'][i] = sub_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_padded['tokens'], data_padded['Level'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded = pad_sequences(X_train, maxlen=120, truncating='post')\n",
    "testing_padded = pad_sequences(X_test, maxlen=120, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6851 - accuracy: 0.1417 - val_loss: 0.6710 - val_accuracy: 0.1333\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.1667 - val_loss: 0.6487 - val_accuracy: 0.1333\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6449 - accuracy: 0.1667 - val_loss: 0.6264 - val_accuracy: 0.1333\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6247 - accuracy: 0.1667 - val_loss: 0.6041 - val_accuracy: 0.1333\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6049 - accuracy: 0.1667 - val_loss: 0.5816 - val_accuracy: 0.1333\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.1667 - val_loss: 0.5595 - val_accuracy: 0.1333\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5647 - accuracy: 0.1667 - val_loss: 0.5375 - val_accuracy: 0.1333\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5450 - accuracy: 0.1667 - val_loss: 0.5153 - val_accuracy: 0.1333\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5248 - accuracy: 0.1667 - val_loss: 0.4933 - val_accuracy: 0.1333\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5051 - accuracy: 0.1667 - val_loss: 0.4711 - val_accuracy: 0.1333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x4ae890790>"
      ]
     },
     "execution_count": 1404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model, evaluate\n",
    "\n",
    "batch = 32\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "num_epochs = 10\n",
    "model.fit(training_padded, y_train, epochs=num_epochs,batch_size=batch, validation_data=(testing_padded, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained word corpus on data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than building our own model, use embeddings generated from a pretrained corpus on Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since tokenization did better for our model, use that for preprocessing\n",
    "data_pre_trained = main_data.copy()\n",
    "data_pre_trained['tokens'] = data_pre_trained['Report'].apply(lambda x: gensim.utils.simple_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "metadata": {},
   "outputs": [],
   "source": [
    " google_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding vectors to an array from trained model, if out of vocabulary (oov) error occurs just add empty numpy array of all zeros\n",
    "\n",
    "for i in range(len(data_pre_trained['tokens'])):\n",
    "    sub_list = []\n",
    "    for j in data_pre_trained['tokens'][i]:\n",
    "        try:\n",
    "            sub_list.append(google_vectors[j])\n",
    "        except KeyError:\n",
    "            sub_list.append(np.zeros(200,))\n",
    "    data_pre_trained['tokens'][i] = sub_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_pre_trained['tokens'], data_pre_trained['Level'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "training_padded = pad_sequences(X_train, maxlen=60, truncating='post')\n",
    "testing_padded = pad_sequences(X_test, maxlen=60, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 12ms/step - loss: -1.4055 - accuracy: 0.1667 - val_loss: -4.0929 - val_accuracy: 0.1333\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: -5.2354 - accuracy: 0.1667 - val_loss: -8.4463 - val_accuracy: 0.1333\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: -9.1619 - accuracy: 0.1667 - val_loss: -12.9220 - val_accuracy: 0.1333\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: -13.1349 - accuracy: 0.1667 - val_loss: -17.5737 - val_accuracy: 0.1333\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: -17.3283 - accuracy: 0.1667 - val_loss: -22.3251 - val_accuracy: 0.1333\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: -21.6444 - accuracy: 0.1667 - val_loss: -27.3446 - val_accuracy: 0.1333\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: -26.2398 - accuracy: 0.1667 - val_loss: -32.5498 - val_accuracy: 0.1333\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: -30.9078 - accuracy: 0.1667 - val_loss: -38.0786 - val_accuracy: 0.1333\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: -35.9510 - accuracy: 0.1667 - val_loss: -43.7113 - val_accuracy: 0.1333\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: -40.9934 - accuracy: 0.1667 - val_loss: -49.6887 - val_accuracy: 0.1333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x4bfdcda30>"
      ]
     },
     "execution_count": 1410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model, evaluate\n",
    "\n",
    "batch = 32\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "num_epochs = 10\n",
    "model.fit(training_padded, y_train, epochs=num_epochs,batch_size=batch, validation_data=(testing_padded, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "progress-bar: 100%|██████████| 300/300 [00:00<00:00, 5242.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Since tokenization did better for our model, use that for preprocessing\n",
    "data_SkLearn= main_data.copy()\n",
    "data_SKLearn = postprocess(data_SkLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_SkLearn['tokens'], data_SkLearn['Level'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "240it [00:00, 444625.87it/s]\n",
      "\n",
      "60it [00:00, 600616.32it/s]\n",
      "\n",
      "100%|██████████| 240/240 [00:00<00:00, 1921055.27it/s]\n",
      "\n",
      "100%|██████████| 240/240 [00:00<00:00, 1981560.94it/s]\n",
      "/Users/andrewsimon/Library/Python/3.8/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "\n",
      "240it [00:00, 5914.27it/s]\n",
      "\n",
      "60it [00:00, 5805.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = labelizeReports(X_train, 'TRAIN')\n",
    "X_test = labelizeReports(X_test, 'TEST')\n",
    "\n",
    "# Fit train model\n",
    "tweet_w2v = Word2Vec(vector_size=200, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(X_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(X_train)], total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.epochs)\n",
    "\n",
    "# Extract word embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "\n",
    "# Data postprocessing with sklearn, creating unit standard deviation\n",
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in tqdm(map(lambda x: x.words, X_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in tqdm(map(lambda x: x.words, X_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 1414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "clf_decision_tree.fit(train_vecs_w2v, y_train)\n",
    "\n",
    "predictions = clf_decision_tree.predict(test_vecs_w2v)\n",
    "\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1554,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1 4 5 5 2 4 4 3 3 3 4 3 3 4 5 3 1 3 3 5 5 5 1 4 4 4 3 3 2 2 5 2 2 2 2 1 4\n 2 3 5 4 2 5 3 2 5 5 4 2 2 1 1 5 2 2 5 5 2 4 3].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb Cell 43\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#Y135sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m classifier\u001b[39m.\u001b[39mfit(train_vecs_w2v, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#Y135sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m predictions_LR \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(test_vecs_w2v)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#Y135sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m score \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mscore(predictions_LR, y_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#Y135sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m score\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 666\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_base.py:447\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    434\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39m        Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[1;32m    448\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    449\u001b[0m         indices \u001b[39m=\u001b[39m (scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_base.py:429\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39mPredict confidence scores for samples.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39m    this class would be predicted.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    427\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 429\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    430\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m    431\u001b[0m \u001b[39mreturn\u001b[39;00m scores\u001b[39m.\u001b[39mravel() \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    884\u001b[0m         )\n\u001b[1;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1 4 5 5 2 4 4 3 3 3 4 3 3 4 5 3 1 3 3 5 5 5 1 4 4 4 3 3 2 2 5 2 2 2 2 1 4\n 2 3 5 4 2 5 3 2 5 5 4 2 2 1 1 5 2 2 5 5 2 4 3].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression( max_iter = 100000)\n",
    "classifier.fit(train_vecs_w2v, y_train)\n",
    "predictions_LR = classifier.predict(test_vecs_w2v)\n",
    "score = classifier.score(predictions_LR, y_test)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166666666666667"
      ]
     },
     "execution_count": 1426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=800, random_state=11).fit(train_vecs_w2v, y_train)\n",
    "\n",
    "predictions = rf.predict(train_vecs_w2v)\n",
    "\n",
    "score = rf.score(test_vecs_w2v, y_test)\n",
    "\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48333333333333334"
      ]
     },
     "execution_count": 1437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth = 6, n_estimators = 1000).fit(train_vecs_w2v, y_train)\n",
    "\n",
    "predictions = xgb.predict(test_vecs_w2v)\n",
    "\n",
    "score = xgb.score(test_vecs_w2v, y_test)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConfusionMatrixDisplay' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb Cell 47\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#Y150sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m disp \u001b[39m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[39m=\u001b[39mcm, display_labels \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#Y150sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m disp\u001b[39m.\u001b[39mplot()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andrewsimon/nlpmaps/temp_test_files/Word2Vec_test.ipynb#Y150sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m disp\u001b[39m.\u001b[39;49mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConfusionMatrixDisplay' object has no attribute 'show'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "len(y_test)\n",
    "cm = confusion_matrix(y_test, predictions_LR)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = [1,2,3,4,5])\n",
    "disp.plot()\n",
    "disp.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
